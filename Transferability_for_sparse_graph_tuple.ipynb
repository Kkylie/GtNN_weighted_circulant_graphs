{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import sympy as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torcheval.metrics.functional import r2_score\n",
    "import architectures as archit\n",
    "import data as da\n",
    "import stable as sta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graph and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vertices = 300 # number of vertices for two circulant graphs\n",
    "num_graph = 2 # two circulant graphs\n",
    "not_moving_probabilities_vector = [0.05,0.05] # weight p for the diagonal element of graph 1 and 2\n",
    "jump_sizes_vector = [1,30] # location of jump l\n",
    "n_samples = 1000 # number of data pair\n",
    "\n",
    "# generate the circulant graph tuple\n",
    "ts = da.cycle_operator_tuple(num_vertices=num_vertices,\n",
    "                    not_moving_probabilities_vector=not_moving_probabilities_vector,\n",
    "                    jump_sizes_vector=jump_sizes_vector)\n",
    "operator_tuple = ts\n",
    "\n",
    "# normalized the graph tuple\n",
    "scale_all = 250\n",
    "ts_normalized = []\n",
    "for ind_g, g in enumerate(operator_tuple):\n",
    "  # scale the norm to closer to 1 for faster convergence\n",
    "  g = g/num_vertices * scale_all\n",
    "  ts_normalized.append(g)\n",
    "  print(torch.linalg.matrix_norm(g,ord = 2))\n",
    "  if torch.linalg.matrix_norm(g,ord = 2) > 1:\n",
    "    g_normalized = g / torch.linalg.matrix_norm(g,ord = 2)\n",
    "    print(\"Graph {:d} has norm {:.4f} bigger than 1. This graph is normalized\".format(ind_g,np.array(torch.linalg.matrix_norm(g,ord = 2))))\n",
    "    ts_normalized[ind_g] = g_normalized\n",
    "operator_tuple_normalized = (ts_normalized[0],ts_normalized[1])\n",
    "\n",
    "# generate data using the circulant graph tuple\n",
    "# y = 0.76*t1@t0@x + 0.33*t0@t1@x + 0.3*t0@t0@t0@x + noise\n",
    "x, y = da.dataLab_cycles(num_vertices=num_vertices,\n",
    "                      not_moving_probabilities_vector=not_moving_probabilities_vector,\n",
    "                      jump_sizes_vector=jump_sizes_vector,\n",
    "                      noise_stdev = 0.1,\n",
    "                      n_samples = n_samples)\n",
    "\n",
    "# train test split\n",
    "test_data_ratio = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=test_data_ratio)\n",
    "print(X_train.shape) # (number of data) * (number of features) * (number of veritces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create small graph tuple for training\n",
    "def create_small_tuple(operator_tuple, num_vertices_small, scale_all = 250):\n",
    "    num_graph = len(operator_tuple)\n",
    "    num_vertices = operator_tuple[0].shape[0]\n",
    "\n",
    "    ts_small = []\n",
    "    for i in range(num_graph):\n",
    "        ts_small.append(torch.zeros(num_vertices_small,num_vertices_small))\n",
    "\n",
    "    for ind_row in range(num_vertices):\n",
    "        for ind_col in range(num_vertices):\n",
    "            for ind_g, g in enumerate(operator_tuple):\n",
    "                if g[ind_row,ind_col] > 1e-8:\n",
    "                    ind_small_row_low = math.floor(ind_row / num_vertices * num_vertices_small)\n",
    "                    ind_small_col_low = math.floor(ind_col / num_vertices * num_vertices_small)\n",
    "\n",
    "                    line_small_row_high = (ind_small_row_low+1) / num_vertices_small\n",
    "                    line_row_low =  ind_row / num_vertices\n",
    "                    line_row_high =  (ind_row+1) / num_vertices\n",
    "\n",
    "                    line_small_col_high = (ind_small_col_low+1) / num_vertices_small\n",
    "                    line_col_low =  ind_col / num_vertices\n",
    "                    line_col_high =  (ind_col+1) / num_vertices\n",
    "\n",
    "                    if line_small_row_high < line_row_high:\n",
    "                        if line_small_col_high < line_col_high:\n",
    "                            ## line cut in the middle for row and column\n",
    "                            ts_small[ind_g][ind_small_row_low,ind_small_col_low] += (line_small_row_high - line_row_low) * (line_small_col_high - line_col_low) * g[ind_row,ind_col] * num_vertices_small * num_vertices_small\n",
    "                            ts_small[ind_g][ind_small_row_low+1,ind_small_col_low] += (line_row_high - line_small_row_high) * (line_small_col_high - line_col_low) * g[ind_row,ind_col] * num_vertices_small * num_vertices_small\n",
    "                            ts_small[ind_g][ind_small_row_low,ind_small_col_low+1] += (line_small_row_high - line_row_low) * (line_col_high - line_small_col_high) * g[ind_row,ind_col] * num_vertices_small * num_vertices_small\n",
    "                            ts_small[ind_g][ind_small_row_low+1,ind_small_col_low+1] += (line_row_high - line_small_row_high) * (line_col_high - line_small_col_high) * g[ind_row,ind_col] * num_vertices_small * num_vertices_small\n",
    "                        else:\n",
    "                            ## line cut in the middle for row\n",
    "                            ts_small[ind_g][ind_small_row_low,ind_small_col_low] += (line_small_row_high - line_row_low) * (1/num_vertices) * g[ind_row,ind_col] * num_vertices_small * num_vertices_small\n",
    "                            ts_small[ind_g][ind_small_row_low+1,ind_small_col_low] += (line_row_high - line_small_row_high) * (1/num_vertices) * g[ind_row,ind_col] * num_vertices_small * num_vertices_small\n",
    "                    else:\n",
    "                        if line_small_col_high < line_col_high:\n",
    "                            ## line cut in the middle for column\n",
    "                            ts_small[ind_g][ind_small_row_low,ind_small_col_low] += (1/num_vertices) * (line_small_col_high - line_col_low) * g[ind_row,ind_col] * num_vertices_small * num_vertices_small\n",
    "                            ts_small[ind_g][ind_small_row_low,ind_small_col_low+1] += (1/num_vertices) * (line_col_high - line_small_col_high) * g[ind_row,ind_col] * num_vertices_small * num_vertices_small\n",
    "                        else:\n",
    "                            ## entirely in a small graph grid\n",
    "                            ts_small[ind_g][ind_small_row_low,ind_small_col_low] += (1/num_vertices) * (1/num_vertices) * g[ind_row,ind_col] * num_vertices_small * num_vertices_small\n",
    "\n",
    "    # normalized the graph tuple\n",
    "    ts_small_normalized = []\n",
    "    for ind_g, g_small in enumerate(ts_small):\n",
    "        g_small = g_small/num_vertices_small * scale_all\n",
    "        ts_small_normalized.append(g_small)\n",
    "        print(torch.linalg.matrix_norm(g_small,ord = 2))\n",
    "        if torch.linalg.matrix_norm(g_small,ord = 2) > 1:\n",
    "            g_small_normalized = g_small / torch.linalg.matrix_norm(g_small,ord = 2)\n",
    "            print(\"Graph {:d} has norm {:.4f} bigger than 1. This graph is normalized\".format(ind_g,np.array(torch.linalg.matrix_norm(g_small,ord = 2))))\n",
    "            ts_small_normalized[ind_g] = g_small_normalized\n",
    "\n",
    "    operator_tuple_small = (ts_small_normalized[0],ts_small_normalized[1])\n",
    "   \n",
    "    return operator_tuple_small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create small size data for training\n",
    "def create_small_data(X_train, y_train, num_vertices_small):\n",
    "    num_vertices = X_train.shape[2]\n",
    "\n",
    "    X_train_small = torch.zeros(X_train.shape[0], X_train.shape[1], num_vertices_small)\n",
    "    y_train_small = torch.zeros(y_train.shape[0], y_train.shape[1], num_vertices_small)\n",
    "\n",
    "    for ind_row in range(num_vertices):\n",
    "        ind_small_row_low = math.floor(ind_row / num_vertices * num_vertices_small)\n",
    "\n",
    "        line_small_row_high = (ind_small_row_low+1) / num_vertices_small\n",
    "        line_row_low =  ind_row / num_vertices\n",
    "        line_row_high =  (ind_row+1) / num_vertices\n",
    "\n",
    "        if line_small_row_high < line_row_high:\n",
    "            ## line cut in the middle\n",
    "            X_train_small[:,:,ind_small_row_low] += (line_small_row_high - line_row_low) * X_train[:,:,ind_row] * num_vertices_small\n",
    "            X_train_small[:,:,ind_small_row_low+1] += (line_row_high - line_small_row_high) * X_train[:,:,ind_row] * num_vertices_small\n",
    "            y_train_small[:,:,ind_small_row_low] += (line_small_row_high - line_row_low) * y_train[:,:,ind_row] * num_vertices_small\n",
    "            y_train_small[:,:,ind_small_row_low+1] += (line_row_high - line_small_row_high) * y_train[:,:,ind_row] * num_vertices_small\n",
    "        else:\n",
    "            ## entirely in a small size graph grid\n",
    "            X_train_small[:,:,ind_small_row_low] += (1/num_vertices) * X_train[:,:,ind_row] * num_vertices_small\n",
    "            y_train_small[:,:,ind_small_row_low] += (1/num_vertices) * y_train[:,:,ind_row] * num_vertices_small\n",
    "\n",
    "    return X_train_small, y_train_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPU if available\n",
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "operator_tuple_device = (operator_tuple_normalized[0].to(device),operator_tuple_normalized[1].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, operator_tuple_train, operator_tuple_test, X_train, y_train, X_test, y_test, \n",
    "                  Penalty_lam = 0, Stable_Penalty = False, constrain_C_Flag = True, constrain_Cj_Flag = True, target_Upr_Cj_vec = 1, target_Upr_C = 1,\n",
    "                  n_epochs = 5000, lr = 0.01, verbose = True, Plot_loss = True):\n",
    "\n",
    "  loss_fcn = nn.MSELoss()\n",
    "  M = model.monomial_word_support\n",
    "\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "  epoch_tr_loss = []\n",
    "  epoch_ts_loss = []\n",
    "  R2_ts=[]\n",
    "  \n",
    "  for epoch in range(n_epochs):\n",
    "    # training\n",
    "    # evaluate monomial for training graph tuple\n",
    "    M.evaluate_at_operator_tuple(operator_tuple = operator_tuple_train)\n",
    "    model.change_monomial_word_support(M)\n",
    "    model.train()\n",
    "    outs_train = model.forward(X_train)\n",
    "    loss = loss_fcn(outs_train, y_train)\n",
    "\n",
    "    # stability penalty\n",
    "    if Stable_Penalty:\n",
    "      loss = loss + Penalty_lam * sta.compute_penalty(model, target_Upr_Cj_vec, Upr_C = target_Upr_C,\n",
    "                                                constrain_C_Flag = constrain_C_Flag, constrain_Cj_Flag = constrain_Cj_Flag)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_tr_loss.append(loss.item())\n",
    "    y_train_reshape = torch.reshape(y_train,(y_train.shape[0],y_train.shape[1]*y_train.shape[2]))\n",
    "    outs_train_reshape = torch.reshape(outs_train,(outs_train.shape[0],outs_train.shape[1]*outs_train.shape[2]))\n",
    "    #R2Score\n",
    "    R2_train = r2_score(outs_train_reshape, y_train_reshape)\n",
    "\n",
    "\n",
    "    # testing\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      # evaluate monomial for testing graph tuple\n",
    "      M.evaluate_at_operator_tuple(operator_tuple = operator_tuple_test)\n",
    "      model.change_monomial_word_support(M)\n",
    "      outs_test = model.forward(X_test)\n",
    "      y_test_reshape = torch.reshape(y_test,(y_test.shape[0],y_test.shape[1]*y_test.shape[2]))\n",
    "      outs_test_reshape = torch.reshape(outs_test,(outs_test.shape[0],outs_test.shape[1]*outs_test.shape[2]))\n",
    "      R2_test = r2_score(outs_test_reshape, y_test_reshape)\n",
    "      GNN_test_loss = loss_fcn(outs_test, y_test)\n",
    "      Penalty_test_loss = 0\n",
    "      # stability penalty\n",
    "      if Stable_Penalty:\n",
    "        Penalty_test_loss = sta.compute_penalty(model, target_Upr_Cj_vec, Upr_C = target_Upr_C,\n",
    "                                            constrain_C_Flag = constrain_C_Flag, constrain_Cj_Flag = constrain_Cj_Flag)\n",
    "      Test_loss = GNN_test_loss + Penalty_lam * Penalty_test_loss\n",
    "      epoch_ts_loss.append(Test_loss)\n",
    "      R2_ts.append(R2_test)\n",
    "\n",
    "\n",
    "    if verbose and (epoch % 10 == 0):\n",
    "      print(\"Epoch {:05d} | Train Loss {:.4f} | Train_R^2 {:.4f} | Test Loss {:.4f} | Test_R^2 {:.4f} | Test GNN loss {:.4f} | Test penalty loss {:.4f} |\".\n",
    "            format(epoch,  epoch_tr_loss[epoch], R2_train, epoch_ts_loss[epoch], R2_test, GNN_test_loss, Penalty_test_loss))\n",
    "\n",
    "\n",
    "  epoch_ts_loss = torch.stack(epoch_ts_loss).cpu()\n",
    "  R2_ts = torch.stack(R2_ts).cpu()\n",
    "\n",
    "  if Plot_loss:\n",
    "    fig = plt.figure()\n",
    "    epoch_seq=np.arange(100, len(epoch_tr_loss) + 1)\n",
    "    plt.plot(epoch_seq, epoch_tr_loss[99:],'.-')\n",
    "    plt.plot(epoch_seq, epoch_ts_loss[99:],'r.-')\n",
    "    plt.grid()\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Curve')\n",
    "    plt.legend(['Training', 'Test'])\n",
    "    plt.show()\n",
    "\n",
    "  return model, R2_test, epoch_ts_loss, R2_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create monomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_variable = 2\n",
    "allowed_degree = 3\n",
    "M = archit.MonomialWordSupport(num_variables=num_variable, allowed_degree = allowed_degree, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-layer GtNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "num_features_in = 1\n",
    "num_features_out = 1\n",
    "Stable_Penalty = False\n",
    "\n",
    "num_vertices_small_all = [100,150,200,250,300]\n",
    "model_GNN_small_all = []\n",
    "result_GNN_small_all = []\n",
    "ts_loss_GNN_small_all = []\n",
    "ts_R2_GNN_small_all = []\n",
    "for num_vertices_small in num_vertices_small_all:\n",
    "    # create small size training graph and data\n",
    "    print(num_vertices_small)\n",
    "    operator_tuple_small = create_small_tuple(operator_tuple, num_vertices_small, scale_all = scale_all)\n",
    "    X_train_small, y_train_small = create_small_data(X_train, y_train, num_vertices_small)\n",
    "    X_train_small = X_train_small.to(device)\n",
    "    y_train_small = y_train_small.to(device)\n",
    "    operator_tuple_small_device = (operator_tuple_small[0].to(device),operator_tuple_small[1].to(device))\n",
    "\n",
    "    # initial evaluate monomial for training graph tuple\n",
    "    M.evaluate_at_operator_tuple(operator_tuple=operator_tuple_small_device)\n",
    "\n",
    "    # 1-layer GtNN model\n",
    "    model_GNN_small = archit.OperatorFilterLayer(num_features_in = num_features_in,\n",
    "                                                num_features_out = num_features_out, monomial_word_support = M)\n",
    "    model_GNN_small.to(device)\n",
    "\n",
    "    # training\n",
    "    model_GNN_small, result_GNN_small, ts_loss_GNN_small, R2_ts_GNN_small = train_network(\n",
    "        model_GNN_small,operator_tuple_small_device, operator_tuple_device, X_train_small, y_train_small, X_test, y_test, \n",
    "        Stable_Penalty = Stable_Penalty, n_epochs = n_epochs)\n",
    "    \n",
    "    model_GNN_small_all.append(model_GNN_small)\n",
    "    result_GNN_small_all.append(result_GNN_small)\n",
    "    ts_loss_GNN_small_all.append(ts_loss_GNN_small)\n",
    "    ts_R2_GNN_small_all.append(R2_ts_GNN_small)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "torch.save(result_GNN_small_all, 'result_GNN_small_all.pt')\n",
    "torch.save(ts_loss_GNN_small_all, 'ts_loss_GNN_small_all.pt')\n",
    "torch.save(ts_R2_GNN_small_all, 'ts_R2_GNN_small_all.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-layer stable GtNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "Penalty_lam = 10\n",
    "Stable_Penalty = True\n",
    "\n",
    "model_stable_small_all = []\n",
    "result_stable_small_all = []\n",
    "ts_loss_stable_small_all = []\n",
    "ts_R2_stable_small_all = []\n",
    "\n",
    "for ind_size, num_vertices_small in enumerate(num_vertices_small_all):\n",
    "  # create small size training graph and data\n",
    "  print(num_vertices_small)\n",
    "  operator_tuple_small = create_small_tuple(operator_tuple, num_vertices_small, scale_all = scale_all)\n",
    "  X_train_small, y_train_small = create_small_data(X_train, y_train, num_vertices_small)\n",
    "  X_train_small = X_train_small.to(device)\n",
    "  y_train_small = y_train_small.to(device)\n",
    "  operator_tuple_small_device = (operator_tuple_small[0].to(device),operator_tuple_small[1].to(device))\n",
    "\n",
    "  # Compute expansion constant for 1-layer GtNN\n",
    "  C_max_sum_small, C_j_max_sum_small = sta.compute_constrain_param(model_GNN_small_all[ind_size])\n",
    "  print(C_max_sum_small)\n",
    "  print(C_j_max_sum_small)\n",
    "  # Set the target expansion constant for stable GtNN as 1/2 of the GtNN\n",
    "  target_Upr_C_small = []\n",
    "  target_Upr_Cj_vec_small = []\n",
    "  for C_max_sum_each_layer in C_max_sum_small:\n",
    "    target_Upr_C_small.append(C_max_sum_each_layer.data/2)\n",
    "  for C_j_max_sum_each_layer in C_j_max_sum_small:\n",
    "    target_Upr_Cj_vec_small.append(C_j_max_sum_each_layer.data/2)\n",
    "  print(\"target:\")\n",
    "  print(target_Upr_C_small)\n",
    "  print(target_Upr_Cj_vec_small)\n",
    "\n",
    "\n",
    "  # initial evaluate monomial for training graph tuple\n",
    "  M.evaluate_at_operator_tuple(operator_tuple=operator_tuple_small_device)\n",
    "\n",
    "  # 1-layer stable GtNN model\n",
    "  model_stable_small = archit.OperatorFilterLayer(num_features_in = num_features_in,\n",
    "                                              num_features_out = num_features_out, monomial_word_support = M)\n",
    "  model_stable_small.to(device)\n",
    "\n",
    "  # training\n",
    "  model_stable_small, result_stable_small, ts_loss_stable_small, R2_ts_stable_small = train_network(\n",
    "      model_stable_small, operator_tuple_small_device, operator_tuple_device, X_train_small, y_train_small, X_test, y_test, \n",
    "      Penalty_lam = Penalty_lam, Stable_Penalty = Stable_Penalty, target_Upr_Cj_vec = target_Upr_Cj_vec_small, target_Upr_C = target_Upr_C_small,\n",
    "      n_epochs = n_epochs)\n",
    "\n",
    "  model_stable_small_all.append(model_stable_small)\n",
    "  result_stable_small_all.append(result_stable_small)\n",
    "  ts_loss_stable_small_all.append(ts_loss_stable_small)\n",
    "  ts_R2_stable_small_all.append(R2_ts_stable_small)\n",
    "\n",
    "  # print expanion constant to check if satisfy constraints\n",
    "  C_max_sum_small_stable, C_j_max_sum_small_stable = sta.compute_constrain_param(model_stable_small)\n",
    "  print(C_max_sum_small_stable)\n",
    "  print(C_j_max_sum_small_stable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "torch.save(result_stable_small_all, 'result_stable_small_all.pt')\n",
    "torch.save(ts_loss_stable_small_all, 'ts_loss_stable_small_all.pt')\n",
    "torch.save(ts_R2_stable_small_all, 'ts_R2_stable_small_all.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_point = 0\n",
    "epoch_seq_all = np.arange(start_point,n_epochs)\n",
    "fig = plt.figure()\n",
    "plt.rc('font',size=15)\n",
    "for ts_loss in ts_loss_GNN_small_all:\n",
    "    plt.plot(epoch_seq_all, ts_loss[start_point:],'-')\n",
    "for ts_loss in ts_loss_stable_small_all:\n",
    "    plt.plot(epoch_seq_all, ts_loss[start_point:],'--')\n",
    "plt.grid()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Testing MSE')\n",
    "plt.ylim([0,0.3])\n",
    "plt.legend(['GtNN:100', 'GtNN:150', 'GtNN:200', 'GtNN:250', 'GtNN:300','ST:100', 'ST:150', 'ST:200', 'ST:250', 'ST:300'])\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.rc('font',size=15)\n",
    "plt.plot(epoch_seq_all, ts_loss_GNN_small_all[0][start_point:],'-')\n",
    "plt.plot(epoch_seq_all, ts_loss_stable_small_all[0][start_point:],'-')\n",
    "plt.grid()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Testing MSE')\n",
    "plt.ylim([0.04,0.2])\n",
    "plt.legend(['GtNN:100','Stable:100'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_point = 0\n",
    "epoch_seq_all = np.arange(start_point,n_epochs)\n",
    "fig = plt.figure()\n",
    "plt.rc('font',size=15)\n",
    "for ts_loss in ts_loss_GNN_small_all:\n",
    "    plt.plot(epoch_seq_all, ts_loss[start_point:],'-')\n",
    "plt.grid()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Testing MSE')\n",
    "plt.ylim([0.005,0.2])\n",
    "plt.legend(['GtNN:100', 'GtNN:150', 'GtNN:200', 'GtNN:250', 'GtNN:300'])\n",
    "plt.show()\n",
    "\n",
    "start_point = 0\n",
    "epoch_seq_all = np.arange(start_point,n_epochs)\n",
    "fig = plt.figure()\n",
    "plt.rc('font',size=15)\n",
    "for ts_loss in ts_loss_stable_small_all:\n",
    "    plt.plot(epoch_seq_all, ts_loss[start_point:],'--')\n",
    "plt.grid()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Testing MSE')\n",
    "plt.ylim([0.005,0.2])\n",
    "plt.legend(['Stable:100', 'Stable:150', 'Stable:200', 'Stable:250', 'Stable:300'],loc='upper left', bbox_to_anchor=(0.08, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_point = 100\n",
    "epoch_seq_all = np.arange(start_point,n_epochs)\n",
    "fig = plt.figure()\n",
    "for R2_ts in ts_R2_GNN_small_all:\n",
    "    plt.plot(epoch_seq_all, R2_ts[start_point:],'-')\n",
    "plt.grid()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Testing $R^2$')\n",
    "plt.legend(['GtNN:100', 'GtNN:150', 'GtNN:200', 'GtNN:250', 'GtNN:300'])\n",
    "plt.show()\n",
    "\n",
    "start_point = 100\n",
    "epoch_seq_all = np.arange(start_point,n_epochs)\n",
    "fig = plt.figure()\n",
    "for R2_ts in ts_R2_stable_small_all:\n",
    "    plt.plot(epoch_seq_all, R2_ts[start_point:],'--')\n",
    "plt.grid()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Testing $R^2$')\n",
    "plt.legend(['Stable:100', 'Stable:150', 'Stable:200', 'Stable:250', 'Stable:300'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(epoch_seq_all, ts_R2_GNN_small_all[0][start_point:],'-')\n",
    "plt.plot(epoch_seq_all, ts_R2_stable_small_all[0][start_point:],'-')\n",
    "plt.grid()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Testing $R^2$')\n",
    "plt.legend(['GtNN:100','Stable:100'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
